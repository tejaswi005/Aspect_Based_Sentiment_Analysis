{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Standard_eval_bert_modified.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e8c900bf324144968cae872899cc4887":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1f895e64a0744eaba4e912bbf089456c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_844008df086c4a06adf0c6742be0a651","IPY_MODEL_57a3f20b1a6f486e9e0ae23db1a6b1ae"]}},"1f895e64a0744eaba4e912bbf089456c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"844008df086c4a06adf0c6742be0a651":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0faf5473b9e843e0abfb3ddce5185b82","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fbf5e47f7e08411ab4d1128e18f4afe1"}},"57a3f20b1a6f486e9e0ae23db1a6b1ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c5d13ec6145244b0939e6a6e8dc528ce","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 301kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_18c383b128f44fe8974b6c47d9d101fb"}},"0faf5473b9e843e0abfb3ddce5185b82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fbf5e47f7e08411ab4d1128e18f4afe1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c5d13ec6145244b0939e6a6e8dc528ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"18c383b128f44fe8974b6c47d9d101fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"9ORmvLibSHwQ","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"71a91796-c015-42c9-9b79-d107862eded9"},"source":["cd drive/My\\ Drive/Neurosyntactic-Model-for-ABSA-master/ATE/POS"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/bert_modified_eval\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3_0Ww-3sSJYN","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"4642deaf-675d-43a3-aae3-9f22c10d69c6"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic":{"type":"string"},"text/plain":["'/content/drive/My Drive/bert_modified_eval'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"R_8aNaQxdjH6","colab":{"base_uri":"https://localhost:8080/","height":406},"outputId":"5a45f772-1383-4a41-fe9f-564999091fa3"},"source":["pip install pytorch_pretrained_bert"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pytorch_pretrained_bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.18)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: botocore<1.18.0,>=1.17.18 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.18)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.18->boto3->pytorch_pretrained_bert) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.18->boto3->pytorch_pretrained_bert) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.18->boto3->pytorch_pretrained_bert) (1.12.0)\n","Installing collected packages: pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aCe62leadtuh","colab":{"base_uri":"https://localhost:8080/","height":608},"outputId":"578219f9-443a-4a04-c2b8-855ea061fdea"},"source":["pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\r\u001b[K     |▍                               | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\r\u001b[K     |▍                               | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 24.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 30.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 18.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 16.9MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 18.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 14.7MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 16.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 15.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 13.8MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 13.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122kB 13.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 194kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 204kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 225kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 235kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 256kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 266kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 276kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 286kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 296kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 307kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 317kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 327kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 337kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 348kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 358kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 368kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 378kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 399kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 409kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 419kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 430kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 440kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 450kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 460kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 471kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 481kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 491kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 501kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 512kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 522kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 532kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 542kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 552kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 563kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 573kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 583kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 593kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 604kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 614kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 624kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 634kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 645kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 655kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 665kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 675kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 686kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 696kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 706kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 716kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 727kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 737kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 747kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 757kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 768kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 778kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 788kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 798kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 808kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 819kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 829kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 839kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 849kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 860kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 870kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 880kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 890kB 13.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 18.5MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 30.1MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=70950f4066f6705fed2b80b2c1684cf526c9a71329f689476188bb66d161acb5\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YtPrfmANYpjP"},"source":["import os\n","import logging\n","import argparse\n","import random\n","import json\n","  \n","import numpy as np\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# from pytorch_pretrained_bert.tokenization import BertTokenizer\n","# from pytorch_pretrained_bert.modeling import PreTrainedBertModel, BertModel\n","from pytorch_pretrained_bert.optimization import BertAdam\n","from model import Net\n","import absa_data_utils as data_utils\n","import absa_data_utils_test as data_utils_test\n","from absa_data_utils import ABSATokenizer\n","# import modelconfig\n","from torch.autograd import Variable"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHAioPveWdiK"},"source":["# logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","#                     datefmt = '%m/%d/%Y %H:%M:%S',\n","#                     level = logging.INFO)\n","# logger = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"69kCp4YeZMg5"},"source":["random.seed(1337)\n","np.random.seed(1337)\n","torch.manual_seed(1337)\n","torch.cuda.manual_seed_all(1337)\n","def warmup_linear(x, warmup=0.002):\n","    if x < warmup:\n","        return x/warmup\n","    return 1.0 - x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"injpHKZbZVgK"},"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import BertTokenizer,BertModel\n","from transformers import AutoModel, AutoTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8JcXw1SaTFh"},"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SY4Rd0Ji8Pnx","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["e8c900bf324144968cae872899cc4887","1f895e64a0744eaba4e912bbf089456c","844008df086c4a06adf0c6742be0a651","57a3f20b1a6f486e9e0ae23db1a6b1ae","0faf5473b9e843e0abfb3ddce5185b82","fbf5e47f7e08411ab4d1128e18f4afe1","c5d13ec6145244b0939e6a6e8dc528ce","18c383b128f44fe8974b6c47d9d101fb"]},"outputId":"c78c7adf-6004-485a-dfc2-7e54ddb47e94"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8c900bf324144968cae872899cc4887","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t6v72wfcE7dM","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"8d85a734-96bd-4f17-a215-03e76523d89a"},"source":["# encode=tokenizer.encode_plus('i am ram',max_length=10,pad_to_max_length=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2VKgbUDwK-Dy"},"source":["data_dir='../laptop'\n","output_dir='output'\n","\n","train_batch_size=16\n","eval_batch_size=8\n","max_seq_length=256\n","do_valid=True\n","learning_rate=3e-5\n","num_train_epochs=8\n","warmup_proportion=0.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qR57sGixM0K7"},"source":["def train():\n","    processor = data_utils.AeProcessor()\n","    label_list = processor.get_labels()\n","    tokenizer = ABSATokenizer.from_pretrained('bert-base-uncased')\n","    train_examples = processor.get_train_examples(data_dir)\n","    train_features = data_utils.convert_examples_to_features(\n","        train_examples, label_list, max_seq_length, tokenizer, \"ae\")\n","    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n","    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n","    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n","    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n","    \n","    train_data = TensorDataset(all_input_ids, all_segment_ids, all_input_mask, all_label_ids)\n","\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n","    # pickle.dump(train_dataloader,open(\"train_32_1.pkl\",\"wb\"))\n","    # train_dataloader=pickle.load(open(\"train.pkl\",\"rb\"))\n","    num_labels=len(label_list)\n","    #>>>>> validation\n","    if do_valid:\n","        valid_examples = processor.get_dev_examples(data_dir)\n","        valid_features=data_utils.convert_examples_to_features(\n","            valid_examples, label_list, max_seq_length, tokenizer, \"ae\")\n","        valid_all_input_ids = torch.tensor([f.input_ids for f in valid_features], dtype=torch.long)\n","        valid_all_segment_ids = torch.tensor([f.segment_ids for f in valid_features], dtype=torch.long)\n","        valid_all_input_mask = torch.tensor([f.input_mask for f in valid_features], dtype=torch.long)\n","        valid_all_label_ids = torch.tensor([f.label_id for f in valid_features], dtype=torch.long)\n","        valid_data = TensorDataset(valid_all_input_ids, valid_all_segment_ids, valid_all_input_mask, valid_all_label_ids)\n","        valid_sampler = SequentialSampler(valid_data)\n","        valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=train_batch_size)  \n","\n","        best_valid_loss=float('inf')\n","        valid_losses=[]  \n","    \n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model = Net(device=device,num_labels=num_labels).cuda()\n","    # model = nn.DataParallel(model)\n","\n","    # Prepare optimizer\n","    param_optimizer = [(k, v) for k, v in model.named_parameters() if v.requires_grad==True]\n","    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","    t_total = len(train_dataloader) * num_train_epochs\n","    optimizer = BertAdam(optimizer_grouped_parameters,\n","                         lr=learning_rate,\n","                         warmup=warmup_proportion,\n","                         t_total=t_total)\n","    \n","    global_step = 0\n","    model.train()\n","    for q in range(num_train_epochs):\n","        for step, batch in enumerate(train_dataloader):\n","            batch = tuple(t.cuda() for t in batch)\n","            input_ids, segment_ids, input_mask, label_ids = batch\n","            _,_,_,_,loss=model(input_ids,input_mask, segment_ids,label_ids)\n","            # loss = model(input_ids, input_mask, segment_ids,label_ids)\n","            loss.backward()\n","\n","            lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = lr_this_step\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            #>>>> perform validation at the end of each epoch .\n","        if do_valid:\n","            model.eval()\n","            with torch.no_grad():\n","                losses=[]\n","                valid_size=0\n","                for step, batch in enumerate(valid_dataloader):\n","                    batch = tuple(t.cuda() for t in batch) # multi-gpu does scattering it-self\n","                    input_ids, segment_ids, input_mask, label_ids = batch\n","                    _,_,_,_,loss = model(input_ids, input_mask, segment_ids,label_ids)\n","                    losses.append(loss.detach().mean().item()*input_ids.size(0) )\n","                    valid_size+=input_ids.size(0)\n","                valid_loss=sum(losses)/valid_size\n","                print(\"validation loss: %f\", valid_loss)\n","                valid_losses.append(valid_loss)\n","            # if valid_loss<best_valid_loss:\n","            if q==2 or q==1 or q==3:\n","                print(\"model saved\",'',q)\n","                torch.save(model, os.path.join(output_dir,str(q), \"model.pt\") )\n","                best_valid_loss=valid_loss\n","            model.train()\n","    if do_valid:\n","        with open(os.path.join(output_dir, \"valid.json\"), \"w\") as fw:\n","            json.dump({\"valid_losses\": valid_losses}, fw)\n","    else:\n","        torch.save(model, os.path.join(output_dir, \"model.pt\") )\n","\n","     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tS7XIxa-RdMJ","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"f733ce34-1cb4-434b-f59d-8ee2396a50b0"},"source":["train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Kc5P16DtTZcW"},"source":["import absa_data_utils_test as data_utils_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Axl2HXohJrnL"},"source":["def test(e):  # Load a trained model that you have fine-tuned (we assume evaluate on cpu)    \n","    processor = data_utils_test.AeProcessor()\n","    label_list = processor.get_labels()\n","    tokenizer = ABSATokenizer.from_pretrained('bert-base-uncased')\n","    eval_examples = processor.get_test_examples(data_dir)\n","    eval_features = data_utils_test.convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer, \"ae\")\n","\n","    print(\"***** Running evaluation *****\")\n","    print(\"  Num examples = %d\", len(eval_examples))\n","    print(\"  Batch size = %d\", eval_batch_size)\n","    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n","    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n","    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n","    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n","    eval_data = TensorDataset(all_input_ids, all_segment_ids, all_input_mask, all_label_ids)\n","    # Run prediction for full data\n","    eval_sampler = SequentialSampler(eval_data)\n","    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n","\n","    model = torch.load(os.path.join(output_dir,str(e), \"model.pt\") )\n","    model.cuda()\n","    model.eval()\n","    \n","    full_logits=[]\n","    full_label_ids=[]\n","    for step, batch in enumerate(eval_dataloader):\n","        batch = tuple(t.cuda() for t in batch)\n","        input_ids, segment_ids, input_mask, label_ids = batch\n","        \n","        with torch.no_grad():\n","            _,logits,_,_,_  = model(input_ids, input_mask, segment_ids,label_ids)\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = label_ids.cpu().numpy()\n","\n","        full_logits.extend(logits.tolist() )\n","        full_label_ids.extend(label_ids.tolist() )\n","\n","    output_eval_json = os.path.join(output_dir,str(e),\"predictions.json\") \n","    with open(output_eval_json, \"w\") as fw:\n","        assert len(full_logits)==len(eval_examples)\n","        #sort by original order for evaluation\n","        recs={}\n","        for qx, ex in enumerate(eval_examples):\n","            recs[int(ex.guid.split(\"-\")[1]) ]={\"sentence\": ex.text_a, \"idx_map\": ex.idx_map, \"logit\": full_logits[qx][ex.start:]} #skip the [CLS] tag.\n","        full_logits=[recs[qx][\"logit\"] for qx in range(len(full_logits))]\n","        raw_X=[recs[qx][\"sentence\"] for qx in range(len(eval_examples) ) ]\n","        idx_map=[recs[qx][\"idx_map\"] for qx in range(len(eval_examples)) ]\n","        json.dump({\"logits\": full_logits, \"raw_X\": raw_X, \"idx_map\": idx_map}, fw)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhwzH3qUT90v"},"source":["for e in range(1,4):\n","  test(e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SGrAcoB-BNNR"},"source":[""],"execution_count":null,"outputs":[]}]}